{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the datasets\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_oil = pd.read_csv('../data/oil.csv')\n",
    "df_holidays_events = pd.read_csv('../data/holidays_events.csv')\n",
    "df_stores = pd.read_csv('../data/stores.csv')\n",
    "df_transactions = pd.read_csv('../data/transactions.csv')\n",
    "# df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "df_holidays_events['date'] = pd.to_datetime(df_holidays_events['date'])\n",
    "df_transactions['date'] = pd.to_datetime(df_transactions['date'])\n",
    "\n",
    "dataframes = {\n",
    "    'Training dataset': df_train,\n",
    "    'Oil price dataset' :df_oil,\n",
    "    'Holidays and events dataset': df_holidays_events,\n",
    "    'Stores dataset': df_stores,\n",
    "    'Transactions dataset': df_transactions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Explore the dataset by displaying the first few rows, summary statistics, and data types of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the first 5 rows\n",
    "**Har valgt å ta de separat fordi det virker som at data clean kommer før integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    display(HTML(f\"<h3 style='color: black;'>{name}</h3>\"))\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the summary statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    display(HTML(f\"<h3 style='color: black;'>{name}</h3>\"))\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    display(HTML(f\"<h3 style='color: black;'>{name}</h3>\"))\n",
    "    display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Identify missing values, outliers, and unique values in categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    display(HTML(f\"<h3 style='color: black;'>{name}</h3>\"))\n",
    "    display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for outliers\n",
    "#### Start with visualizing the numerical data to get a picture of how the data looks in terms of distribution\n",
    "##### Sales & promotion:\n",
    "\"Since the distribution is skewed with a very long tail, it's clear that there are outliers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'dataframes' is a dictionary of DataFrames with the key as DataFrame name\n",
    "# Replace this with your actual DataFrame loading code\n",
    "# dataframes = {'df_train': df_train, ...}\n",
    "\n",
    "numerical_columns_of_interest = ['sales', 'onpromotion', 'transactions', 'dcoilwtico']\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    valid_columns = [col for col in numerical_columns_of_interest if col in df.columns]\n",
    "\n",
    "    if valid_columns:\n",
    "        num_columns = len(valid_columns)\n",
    "        \n",
    "        # Adjust the axes based on number of columns\n",
    "        fig, axes = plt.subplots(nrows=num_columns, ncols=3, figsize=(18, num_columns * 5))\n",
    "        fig.tight_layout(pad=5.0)\n",
    "        \n",
    "        # If there's only one numerical column, axes will be 1D, otherwise 2D\n",
    "        if num_columns == 1:\n",
    "            axes = [axes]  # Convert to a list for consistency in indexing\n",
    "\n",
    "        for i, column in enumerate(valid_columns):\n",
    "            # Histogram\n",
    "            sns.histplot(df[column], bins=100, ax=axes[i][0])\n",
    "            axes[i][0].set_title(f'Distribution of {column}')\n",
    "            axes[i][0].set_xlabel(column)\n",
    "            axes[i][0].set_ylabel('Frequency')\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(x=df[column], ax=axes[i][1])\n",
    "            axes[i][1].set_title(f'Boxplot of {column}')\n",
    "            axes[i][1].set_xlabel(column)\n",
    "\n",
    "            # Time series (average grouped by 'date')\n",
    "            df_grouped = df.groupby('date')[column].mean().reset_index()  # Group by date and aggregate by mean\n",
    "            sns.lineplot(data=df_grouped, x='date', y=column, ax=axes[i][2], color='purple')\n",
    "            axes[i][2].set_title(f'Time Series (Avg) per day of {column}')\n",
    "            axes[i][2].set_xlabel('Date')\n",
    "            axes[i][2].set_ylabel(f'Avg {column}')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for unique values in categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    display(HTML(f\"<h3 style='color: black;'>{name}</h3>\"))\n",
    "    display(\n",
    "        HTML(\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"Column Name\": column,\n",
    "                        \"Unique Values\": \", \".join(map(str, df[column].unique())),\n",
    "                    }\n",
    "                    for column in df.select_dtypes(include=[\"object\"]).columns if column != 'date'\n",
    "                ]\n",
    "            ).to_html(index=False)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Handling Missing Values\n",
    "## b. Choose appropriate methods to handle missing values (e.g., mean/median imputation for numerical data, mode imputation for categorical data, or deletion of rows/columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er kun oil price som har missing values, og den har kun 1219 rader. \n",
    "Dette går nok innunder kategorien 'lite datasett', og man kan dermed ikke bare fjerne radene med manglende verdier. Dette kommer til å fjerne 5% av datasettet. \n",
    "\n",
    "Da blir det \"imputation\", narurlig med mean eller median. [Denne](https://www.kaggle.com/code/pagenotfound/mean-and-median-imputation) artikkelen sier at mean brukes når fordelingen er normalfordelt, eller median. Basert på histogrammet er det tydelig at fordelingen ikke er normalfordelt, så da blir det median. Det gir iidlertid denne utviklingen, som ikke ser helt realistisk ut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_copy = df_oil.copy()\n",
    "median_value = df_oil_copy['dcoilwtico'].median()\n",
    "original_dcoilwtico = df_oil_copy['dcoilwtico'].copy()\n",
    "df_oil_copy['dcoilwtico'] = df_oil_copy['dcoilwtico'].fillna(median_value)\n",
    "filled_mask = original_dcoilwtico.isnull()\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=df_oil_copy, x='date', y='dcoilwtico', color='blue', label='dcoilwtico', marker='', linestyle='-')\n",
    "plt.scatter(df_oil_copy.loc[filled_mask, 'date'], \n",
    "    df_oil_copy.loc[filled_mask, 'dcoilwtico'], \n",
    "    color='red', label='Filled Values', s=50\n",
    ")\n",
    "plt.title('Time Series Plot of dcoilwtico with Filled Values Highlighted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('dcoilwtico')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_copy = df_oil.copy()\n",
    "original_dcoilwtico = df_oil_copy['dcoilwtico'].copy()\n",
    "df_oil_copy['dcoilwtico'] = df_oil_copy['dcoilwtico'].interpolate()\n",
    "filled_mask = original_dcoilwtico.isnull()\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=df_oil_copy, x='date', y='dcoilwtico', color='blue', label='dcoilwtico', marker='', linestyle='-')\n",
    "plt.scatter(df_oil_copy.loc[filled_mask, 'date'], \n",
    "            df_oil_copy.loc[filled_mask, 'dcoilwtico'], \n",
    "            color='red', label='Filled Values', s=50)\n",
    "plt.title('Time Series Plot of dcoilwtico with Filled Values Highlighted (Linear Interpolation)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('dcoilwtico')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Justify your choices for handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basert på timeseries'ene, så virker det svært lite realitsik at de verdiene vi erstatter faktisk er median-verdier, eller noe i nærheten. Ved å plott opp utfyllingen ved å heller bruke lineær itnerpolasjon, ser det ut til at de innfylte dataene passer mye bedre inn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Detect outliers using methods such as the IQR method or Z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices = []\n",
    "\n",
    "grouped = df_train.groupby(['store_nbr', 'family'])\n",
    "\n",
    "# Define a function to process each group\n",
    "def process_group(group, group_name):\n",
    "    global outlier_indices  # Allow access to the outer scope variable\n",
    "\n",
    "    # Set the date as the index and resample\n",
    "    group = group.set_index('date').resample('D').sum()  # Daily resampling\n",
    "    \n",
    "    # Print the current group being processed\n",
    "    print(f\"\\nProcessing Group: Store {group_name[0]}, Family: {group_name[1]}\")\n",
    "\n",
    "    # Check for missing values\n",
    "    if group['sales'].isnull().any():\n",
    "        print(f\"Missing values detected in group: {group_name}\")\n",
    "\n",
    "    # Replace zeros with NaN and fill NaNs with forward fill\n",
    "    group['sales'] = group['sales'].replace(0, np.nan)\n",
    "    group['sales'] = group['sales'].ffill()  # Forward fill to handle NaN sales\n",
    "\n",
    "    # Drop any remaining NaN values if present after filling\n",
    "    group = group.dropna(subset=['sales'])\n",
    "\n",
    "    # Ensure enough data for decomposition\n",
    "    if len(group['sales']) >= 30:  # At least 30 data points needed\n",
    "        # Decompose the time series\n",
    "        try:\n",
    "            decomposition = seasonal_decompose(group['sales'], model='additive', period=int(365/4))  # Adjust period as needed\n",
    "\n",
    "            fig, axes = plt.subplots(4, 1, figsize=(12, 12))\n",
    "            decomposition.observed.plot(ax=axes[0], title='Observed', legend=False)\n",
    "            decomposition.trend.plot(ax=axes[1], title='Trend', legend=False)\n",
    "            decomposition.seasonal.plot(ax=axes[2], title='Seasonal', legend=False)\n",
    "            decomposition.resid.plot(ax=axes[3], title='Residual', legend=False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate IQR for residuals\n",
    "            residuals = decomposition.resid.dropna()\n",
    "            Q1 = residuals.quantile(0.25)\n",
    "            Q3 = residuals.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Define bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Identify outliers\n",
    "            outliers = residuals[(residuals < lower_bound) | (residuals > upper_bound)]\n",
    "            outlier_indices.extend(outliers.index)  # Collect outlier indices\n",
    "\n",
    "            # Plotting residuals and highlight outliers (optional)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(residuals.index, residuals, label='Residuals', color='blue')\n",
    "            plt.scatter(outliers.index, outliers, color='red', label='Outliers')\n",
    "            plt.axhline(y=lower_bound, color='red', linestyle='--', label='Lower Bound')\n",
    "            plt.axhline(y=upper_bound, color='green', linestyle='--', label='Upper Bound')\n",
    "            plt.title(f'Residuals for Store {group_name[0]}, Family: {group_name[1]} with Outliers Highlighted')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        except ValueError as e:\n",
    "            print(f\"Decomposition failed for group: {group_name}, error: {e}\")\n",
    "    else:\n",
    "        print(f\"Not enough data to decompose in group: {group_name}\")\n",
    "\n",
    "# Apply processing function to each group\n",
    "for name, group in grouped:\n",
    "    process_group(group, name)\n",
    "    \n",
    "outlier_indices_set = set(outlier_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Decide whether to remove, cap, or transform the outliers. Justify your decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train[~df_train.index.isin(outlier_indices_set)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
